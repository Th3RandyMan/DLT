{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7940fa9",
   "metadata": {},
   "source": [
    "# Production Deployment Guide\n",
    "\n",
    "This notebook demonstrates how to deploy DLT models in production environments, including model serialization, deployment strategies, monitoring, and best practices for production ML systems.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Model Serialization and Persistence](#serialization)\n",
    "2. [Production Configuration](#config)\n",
    "3. [Model Serving Strategies](#serving)\n",
    "4. [Monitoring and Logging](#monitoring)\n",
    "5. [Performance Optimization](#optimization)\n",
    "6. [CI/CD Integration](#cicd)\n",
    "7. [Best Practices](#best-practices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "466dbd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production deployment dependencies loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import joblib\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to Python path\n",
    "sys.path.append('/home/rlfowler/Documents/myprojects/DLT/src')\n",
    "\n",
    "from dlt.core.config import DLTConfig\n",
    "from dlt.core.model import DLTModel\n",
    "from dlt.core.pipeline import train, evaluate, predict\n",
    "from dlt.core.trainer import DLTTrainer\n",
    "\n",
    "print(\"Production deployment dependencies loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f1d91e",
   "metadata": {},
   "source": [
    "## 1. Model Serialization and Persistence {#serialization}\n",
    "\n",
    "Proper model serialization is crucial for production deployment. We'll explore different serialization methods and their trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbaab75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample model created for deployment demonstration\n"
     ]
    }
   ],
   "source": [
    "# Create a sample model for demonstration\n",
    "config = DLTConfig(\n",
    "    model_type=\"sklearn.ensemble.RandomForestClassifier\",\n",
    "    model_params={\n",
    "        \"n_estimators\": 100,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    ")\n",
    "\n",
    "# Update with additional configuration\n",
    "config.training.update({\n",
    "    'epochs': 5,\n",
    "    'validation_split': 0.2\n",
    "})\n",
    "\n",
    "config.data.update({\n",
    "    'input_shape': [784],\n",
    "    'output_shape': [10],\n",
    "    'batch_size': 32\n",
    "})\n",
    "\n",
    "model = DLTModel.from_config(config)\n",
    "print(\"Sample model created for deployment demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a2277f",
   "metadata": {},
   "source": [
    "### Model Serialization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1507ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved using joblib: /home/rlfowler/Documents/myprojects/DLT/models/production/model.joblib\n",
      "Model metadata saved: /home/rlfowler/Documents/myprojects/DLT/models/production/metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Create models directory for production artifacts\n",
    "models_dir = Path('/home/rlfowler/Documents/myprojects/DLT/models/production')\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Method 1: Framework-specific serialization (recommended)\n",
    "def save_model_framework_native(model, path):\n",
    "    \"\"\"Save model using framework's native serialization.\"\"\"\n",
    "    if hasattr(model, 'save_model'):\n",
    "        model.save_model(path)\n",
    "        print(f\"Model saved using framework-native method: {path}\")\n",
    "    else:\n",
    "        print(\"Framework-native save not available\")\n",
    "\n",
    "# Method 2: Joblib serialization (fast, good for sklearn models)\n",
    "def save_model_joblib(model, path):\n",
    "    \"\"\"Save model using joblib.\"\"\"\n",
    "    joblib.dump(model, path)\n",
    "    print(f\"Model saved using joblib: {path}\")\n",
    "\n",
    "# Method 3: Pickle serialization (universal but less efficient)\n",
    "def save_model_pickle(model, path):\n",
    "    \"\"\"Save model using pickle.\"\"\"\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Model saved using pickle: {path}\")\n",
    "\n",
    "# Save model metadata\n",
    "def save_model_metadata(config, path):\n",
    "    \"\"\"Save model configuration and metadata.\"\"\"\n",
    "    metadata = {\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'config': config.model_dump() if hasattr(config, 'model_dump') else str(config),\n",
    "        'version': '1.0.0',\n",
    "        'framework': config.model_type if hasattr(config, 'model_type') else 'unknown'\n",
    "    }\n",
    "    \n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"Model metadata saved: {path}\")\n",
    "\n",
    "# Save the model and metadata\n",
    "model_path = models_dir / 'model.joblib'\n",
    "metadata_path = models_dir / 'metadata.json'\n",
    "\n",
    "save_model_joblib(model, model_path)\n",
    "save_model_metadata(config, metadata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1410a8e7",
   "metadata": {},
   "source": [
    "## 2. Production Configuration {#config}\n",
    "\n",
    "Production environments require different configurations than development. Let's set up production-ready configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ae21806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production configuration saved: /home/rlfowler/Documents/myprojects/DLT/config/config_production.yaml\n",
      "\n",
      "Key production settings:\n",
      "  environment: str\n",
      "  logging: dict\n",
      "  model_serving: dict\n",
      "  monitoring: dict\n",
      "  security: dict\n",
      "  data_validation: dict\n"
     ]
    }
   ],
   "source": [
    "# Production configuration template\n",
    "production_config = {\n",
    "    'environment': 'production',\n",
    "    'logging': {\n",
    "        'level': 'INFO',\n",
    "        'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        'handlers': ['file', 'console'],\n",
    "        'file_path': '/var/log/dlt/production.log'\n",
    "    },\n",
    "    'model_serving': {\n",
    "        'batch_size': 64,\n",
    "        'max_latency_ms': 100,\n",
    "        'timeout_seconds': 30,\n",
    "        'workers': 4\n",
    "    },\n",
    "    'monitoring': {\n",
    "        'enable_metrics': True,\n",
    "        'metrics_endpoint': '/metrics',\n",
    "        'health_endpoint': '/health',\n",
    "        'log_predictions': False,  # Privacy consideration\n",
    "        'alert_thresholds': {\n",
    "            'latency_p95_ms': 200,\n",
    "            'error_rate_percent': 5.0,\n",
    "            'throughput_rps': 100\n",
    "        }\n",
    "    },\n",
    "    'security': {\n",
    "        'enable_auth': True,\n",
    "        'api_key_header': 'X-API-Key',\n",
    "        'rate_limit_per_minute': 1000\n",
    "    },\n",
    "    'data_validation': {\n",
    "        'enable_input_validation': True,\n",
    "        'enable_output_validation': True,\n",
    "        'drift_detection': True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save production config\n",
    "config_path = Path('/home/rlfowler/Documents/myprojects/DLT/config/config_production.yaml')\n",
    "\n",
    "import yaml\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(production_config, f, default_flow_style=False, indent=2)\n",
    "\n",
    "print(f\"Production configuration saved: {config_path}\")\n",
    "print(\"\\nKey production settings:\")\n",
    "for key, value in production_config.items():\n",
    "    print(f\"  {key}: {type(value).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aec00a",
   "metadata": {},
   "source": [
    "## 3. Model Serving Strategies {#serving}\n",
    "\n",
    "Different deployment strategies for serving ML models in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e4e34a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 09:48:41,319 - __main__ - ERROR - Prediction error: This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model API server initialized\n",
      "\n",
      "Test prediction result: {'error': \"This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\"}\n",
      "\n",
      "Health check: {'status': 'healthy', 'timestamp': '2025-09-29T09:48:41.320919', 'model_loaded': True}\n"
     ]
    }
   ],
   "source": [
    "# Strategy 1: REST API Service\n",
    "class ModelAPIServer:\n",
    "    \"\"\"Simple REST API server for model serving.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, config):\n",
    "        self.model = joblib.load(model_path)\n",
    "        self.config = config\n",
    "        self.setup_logging()\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(\n",
    "            level=getattr(logging, self.config.get('logging', {}).get('level', 'INFO')),\n",
    "            format=self.config.get('logging', {}).get('format', '%(message)s')\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"Make predictions on input data.\"\"\"\n",
    "        try:\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            # Validate input\n",
    "            if not self.validate_input(data):\n",
    "                return {'error': 'Invalid input data'}\n",
    "            \n",
    "            # Make prediction\n",
    "            if hasattr(self.model, 'predict'):\n",
    "                predictions = self.model.predict(data)\n",
    "            else:\n",
    "                predictions = \"Mock prediction for demonstration\"\n",
    "            \n",
    "            # Calculate latency\n",
    "            latency_ms = (datetime.now() - start_time).total_seconds() * 1000\n",
    "            \n",
    "            self.logger.info(f\"Prediction completed in {latency_ms:.2f}ms\")\n",
    "            \n",
    "            return {\n",
    "                'predictions': predictions.tolist() if hasattr(predictions, 'tolist') else predictions,\n",
    "                'latency_ms': latency_ms,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Prediction error: {str(e)}\")\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def validate_input(self, data):\n",
    "        \"\"\"Validate input data format.\"\"\"\n",
    "        # Basic validation - expand based on your needs\n",
    "        if data is None:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def health_check(self):\n",
    "        \"\"\"Health check endpoint.\"\"\"\n",
    "        return {\n",
    "            'status': 'healthy',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model_loaded': self.model is not None\n",
    "        }\n",
    "\n",
    "# Initialize API server\n",
    "api_server = ModelAPIServer(model_path, production_config)\n",
    "print(\"Model API server initialized\")\n",
    "\n",
    "# Test the server\n",
    "test_data = [[0.5, 0.3, 0.8, 0.1]]  # Sample input\n",
    "result = api_server.predict(test_data)\n",
    "print(f\"\\nTest prediction result: {result}\")\n",
    "\n",
    "health_status = api_server.health_check()\n",
    "print(f\"\\nHealth check: {health_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f7f22fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch: input.csv -> output.csv\n",
      "Batch size: 64\n",
      "Batch processing result: {'status': 'completed', 'processed_records': 1000, 'processing_time_seconds': 45.2, 'output_file': 'output.csv'}\n"
     ]
    }
   ],
   "source": [
    "# Strategy 2: Batch Prediction Service\n",
    "class BatchPredictionService:\n",
    "    \"\"\"Service for processing batch predictions.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, config):\n",
    "        self.model = joblib.load(model_path)\n",
    "        self.config = config\n",
    "        self.batch_size = config.get('model_serving', {}).get('batch_size', 64)\n",
    "    \n",
    "    def process_batch(self, input_file, output_file):\n",
    "        \"\"\"Process a batch of data from file.\"\"\"\n",
    "        try:\n",
    "            # Simulate batch processing\n",
    "            print(f\"Processing batch: {input_file} -> {output_file}\")\n",
    "            print(f\"Batch size: {self.batch_size}\")\n",
    "            \n",
    "            # In real implementation, you would:\n",
    "            # 1. Read data from input_file\n",
    "            # 2. Process in batches\n",
    "            # 3. Write results to output_file\n",
    "            \n",
    "            return {\n",
    "                'status': 'completed',\n",
    "                'processed_records': 1000,  # Mock value\n",
    "                'processing_time_seconds': 45.2,  # Mock value\n",
    "                'output_file': output_file\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'status': 'failed', 'error': str(e)}\n",
    "\n",
    "# Initialize batch service\n",
    "batch_service = BatchPredictionService(model_path, production_config)\n",
    "result = batch_service.process_batch('input.csv', 'output.csv')\n",
    "print(f\"Batch processing result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132eadb7",
   "metadata": {},
   "source": [
    "## 4. Monitoring and Logging {#monitoring}\n",
    "\n",
    "Production ML systems require comprehensive monitoring and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a50d95b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating production traffic...\n",
      "\n",
      "Metrics after 1 predictions:\n",
      "  Success rate: 100.0%\n",
      "  Mean latency: 147.9ms\n",
      "  P95 latency: 147.9ms\n",
      "ALERT: High P95 latency: 298.31ms > 200ms\n",
      "ALERT: High P95 latency: 276.69ms > 200ms\n",
      "ALERT: High P95 latency: 276.69ms > 200ms\n",
      "ALERT: High P95 latency: 276.69ms > 200ms\n",
      "ALERT: High P95 latency: 276.69ms > 200ms\n",
      "ALERT: High P95 latency: 276.69ms > 200ms\n",
      "ALERT: High P95 latency: 276.69ms > 200ms\n",
      "ALERT: High P95 latency: 276.69ms > 200ms\n",
      "ALERT: High P95 latency: 276.69ms > 200ms\n",
      "ALERT: High P95 latency: 276.69ms > 200ms\n",
      "ALERT: High P95 latency: 276.69ms > 200ms\n",
      "ALERT: High P95 latency: 276.69ms > 200ms\n",
      "ALERT: High P95 latency: 276.69ms > 200ms\n",
      "ALERT: High P95 latency: 276.69ms > 200ms\n",
      "ALERT: High P95 latency: 276.69ms > 200ms\n",
      "ALERT: High P95 latency: 276.69ms > 200ms\n",
      "ALERT: High P95 latency: 276.69ms > 200ms\n",
      "ALERT: High P95 latency: 282.59ms > 200ms\n",
      "ALERT: High P95 latency: 282.59ms > 200ms\n",
      "ALERT: High P95 latency: 282.59ms > 200ms\n",
      "ALERT: High P95 latency: 282.59ms > 200ms\n",
      "ALERT: High P95 latency: 276.69ms > 200ms\n",
      "ALERT: High P95 latency: 276.69ms > 200ms\n",
      "ALERT: High P95 latency: 276.69ms > 200ms\n",
      "ALERT: High P95 latency: 282.59ms > 200ms\n",
      "ALERT: High P95 latency: 282.59ms > 200ms\n",
      "ALERT: High P95 latency: 282.59ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 290.30ms > 200ms\n",
      "ALERT: High P95 latency: 290.30ms > 200ms\n",
      "ALERT: High P95 latency: 290.30ms > 200ms\n",
      "ALERT: High P95 latency: 290.30ms > 200ms\n",
      "\n",
      "Metrics after 51 predictions:\n",
      "  Success rate: 98.0%\n",
      "  Mean latency: 184.7ms\n",
      "  P95 latency: 290.3ms\n",
      "ALERT: High P95 latency: 290.30ms > 200ms\n",
      "ALERT: High P95 latency: 290.30ms > 200ms\n",
      "ALERT: High P95 latency: 290.30ms > 200ms\n",
      "ALERT: High P95 latency: 290.30ms > 200ms\n",
      "ALERT: High P95 latency: 290.30ms > 200ms\n",
      "ALERT: High P95 latency: 290.30ms > 200ms\n",
      "ALERT: High P95 latency: 290.30ms > 200ms\n",
      "ALERT: High P95 latency: 290.30ms > 200ms\n",
      "ALERT: High P95 latency: 290.30ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 290.30ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 284.95ms > 200ms\n",
      "\n",
      "Metrics after 101 predictions:\n",
      "  Success rate: 97.0%\n",
      "  Mean latency: 173.8ms\n",
      "  P95 latency: 284.9ms\n",
      "ALERT: High P95 latency: 284.95ms > 200ms\n",
      "ALERT: High P95 latency: 285.10ms > 200ms\n",
      "ALERT: High P95 latency: 285.10ms > 200ms\n",
      "ALERT: High P95 latency: 285.10ms > 200ms\n",
      "ALERT: High P95 latency: 285.10ms > 200ms\n",
      "ALERT: High P95 latency: 285.10ms > 200ms\n",
      "ALERT: High P95 latency: 285.10ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 290.30ms > 200ms\n",
      "ALERT: High P95 latency: 290.30ms > 200ms\n",
      "ALERT: High P95 latency: 290.30ms > 200ms\n",
      "ALERT: High P95 latency: 290.30ms > 200ms\n",
      "ALERT: High P95 latency: 290.30ms > 200ms\n",
      "ALERT: High P95 latency: 290.30ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 286.72ms > 200ms\n",
      "ALERT: High P95 latency: 285.10ms > 200ms\n",
      "ALERT: High P95 latency: 285.10ms > 200ms\n",
      "ALERT: High P95 latency: 285.10ms > 200ms\n",
      "ALERT: High P95 latency: 285.10ms > 200ms\n",
      "ALERT: High P95 latency: 285.10ms > 200ms\n",
      "ALERT: High P95 latency: 285.10ms > 200ms\n",
      "ALERT: High P95 latency: 285.10ms > 200ms\n",
      "ALERT: High P95 latency: 285.10ms > 200ms\n",
      "ALERT: High P95 latency: 285.10ms > 200ms\n",
      "ALERT: High P95 latency: 285.10ms > 200ms\n",
      "\n",
      "=== Final Production Metrics ===\n",
      "{\n",
      "  \"total_predictions\": 150,\n",
      "  \"success_rate\": 96.66666666666667,\n",
      "  \"error_rate\": 3.3333333333333335,\n",
      "  \"latency_stats\": {\n",
      "    \"mean_ms\": 172.33269255486172,\n",
      "    \"p50_ms\": 175.94373663847833,\n",
      "    \"p95_ms\": 285.0987763657706,\n",
      "    \"p99_ms\": 296.3366971837229\n",
      "  },\n",
      "  \"alert_count\": 131,\n",
      "  \"last_updated\": \"2025-09-29T09:48:50.649122\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import defaultdict, deque\n",
    "from threading import Lock\n",
    "\n",
    "class ProductionMonitor:\n",
    "    \"\"\"Monitor for tracking production metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.metrics = defaultdict(list)\n",
    "        self.alerts = []\n",
    "        self.lock = Lock()\n",
    "        \n",
    "        # Initialize metric storage\n",
    "        self.latencies = deque(maxlen=1000)  # Keep last 1000 latencies\n",
    "        self.error_count = 0\n",
    "        self.success_count = 0\n",
    "        self.prediction_count = 0\n",
    "    \n",
    "    def record_prediction(self, latency_ms, success=True):\n",
    "        \"\"\"Record a prediction event.\"\"\"\n",
    "        with self.lock:\n",
    "            self.prediction_count += 1\n",
    "            self.latencies.append(latency_ms)\n",
    "            \n",
    "            if success:\n",
    "                self.success_count += 1\n",
    "            else:\n",
    "                self.error_count += 1\n",
    "            \n",
    "            # Check for alerts\n",
    "            self.check_alerts(latency_ms, success)\n",
    "    \n",
    "    def check_alerts(self, latency_ms, success):\n",
    "        \"\"\"Check if any alert thresholds are exceeded.\"\"\"\n",
    "        thresholds = self.config.get('monitoring', {}).get('alert_thresholds', {})\n",
    "        \n",
    "        # Latency alert\n",
    "        p95_threshold = thresholds.get('latency_p95_ms', 200)\n",
    "        if len(self.latencies) >= 20:  # Need sufficient data\n",
    "            latencies_sorted = sorted(self.latencies)\n",
    "            p95_latency = latencies_sorted[int(0.95 * len(latencies_sorted))]\n",
    "            if p95_latency > p95_threshold:\n",
    "                self.add_alert(f\"High P95 latency: {p95_latency:.2f}ms > {p95_threshold}ms\")\n",
    "        \n",
    "        # Error rate alert\n",
    "        if self.prediction_count >= 100:  # Need sufficient data\n",
    "            error_rate = (self.error_count / self.prediction_count) * 100\n",
    "            error_threshold = thresholds.get('error_rate_percent', 5.0)\n",
    "            if error_rate > error_threshold:\n",
    "                self.add_alert(f\"High error rate: {error_rate:.2f}% > {error_threshold}%\")\n",
    "    \n",
    "    def add_alert(self, message):\n",
    "        \"\"\"Add an alert.\"\"\"\n",
    "        alert = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'message': message,\n",
    "            'severity': 'warning'\n",
    "        }\n",
    "        self.alerts.append(alert)\n",
    "        print(f\"ALERT: {message}\")\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"Get current metrics summary.\"\"\"\n",
    "        with self.lock:\n",
    "            if not self.latencies:\n",
    "                return {'status': 'no_data'}\n",
    "            \n",
    "            latencies_sorted = sorted(self.latencies)\n",
    "            \n",
    "            return {\n",
    "                'total_predictions': self.prediction_count,\n",
    "                'success_rate': (self.success_count / self.prediction_count) * 100 if self.prediction_count > 0 else 0,\n",
    "                'error_rate': (self.error_count / self.prediction_count) * 100 if self.prediction_count > 0 else 0,\n",
    "                'latency_stats': {\n",
    "                    'mean_ms': sum(self.latencies) / len(self.latencies),\n",
    "                    'p50_ms': latencies_sorted[int(0.5 * len(latencies_sorted))],\n",
    "                    'p95_ms': latencies_sorted[int(0.95 * len(latencies_sorted))],\n",
    "                    'p99_ms': latencies_sorted[int(0.99 * len(latencies_sorted))],\n",
    "                },\n",
    "                'alert_count': len(self.alerts),\n",
    "                'last_updated': datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "# Initialize monitor\n",
    "monitor = ProductionMonitor(production_config)\n",
    "\n",
    "# Simulate some predictions with monitoring\n",
    "import random\n",
    "\n",
    "print(\"Simulating production traffic...\")\n",
    "for i in range(150):\n",
    "    # Simulate varying latencies and occasional failures\n",
    "    latency = random.uniform(50, 300)  # ms\n",
    "    success = random.random() > 0.02  # 2% error rate\n",
    "    \n",
    "    monitor.record_prediction(latency, success)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        metrics = monitor.get_metrics()\n",
    "        print(f\"\\nMetrics after {i+1} predictions:\")\n",
    "        print(f\"  Success rate: {metrics['success_rate']:.1f}%\")\n",
    "        print(f\"  Mean latency: {metrics['latency_stats']['mean_ms']:.1f}ms\")\n",
    "        print(f\"  P95 latency: {metrics['latency_stats']['p95_ms']:.1f}ms\")\n",
    "\n",
    "# Final metrics\n",
    "final_metrics = monitor.get_metrics()\n",
    "print(\"\\n=== Final Production Metrics ===\")\n",
    "print(json.dumps(final_metrics, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e6fc8d",
   "metadata": {},
   "source": [
    "## 5. Performance Optimization {#optimization}\n",
    "\n",
    "Techniques for optimizing model performance in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93c86a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking model with 100 iterations...\n",
      "=== Latency Benchmark Results ===\n",
      "mean_latency_ms: 1.08\n",
      "median_latency_ms: 1.08\n",
      "p95_latency_ms: 1.11\n",
      "p99_latency_ms: 1.13\n",
      "min_latency_ms: 1.05\n",
      "max_latency_ms: 1.13\n",
      "Optimizing batch size...\n",
      "Testing batch size: 1\n",
      "Testing batch size: 8\n",
      "Testing batch size: 16\n",
      "Testing batch size: 32\n",
      "Testing batch size: 32\n",
      "Testing batch size: 64\n",
      "Testing batch size: 64\n",
      "\n",
      "=== Batch Size Optimization ===\n",
      "Optimal batch size: 64\n",
      "Optimal throughput: 997.65 samples/sec\n",
      "\n",
      "Throughput by batch size:\n",
      "  Batch 1: 924.06 samples/sec\n",
      "  Batch 8: 989.61 samples/sec\n",
      "  Batch 16: 994.83 samples/sec\n",
      "  Batch 32: 997.20 samples/sec\n",
      "  Batch 64: 997.65 samples/sec\n",
      "\n",
      "=== Batch Size Optimization ===\n",
      "Optimal batch size: 64\n",
      "Optimal throughput: 997.65 samples/sec\n",
      "\n",
      "Throughput by batch size:\n",
      "  Batch 1: 924.06 samples/sec\n",
      "  Batch 8: 989.61 samples/sec\n",
      "  Batch 16: 994.83 samples/sec\n",
      "  Batch 32: 997.20 samples/sec\n",
      "  Batch 64: 997.65 samples/sec\n"
     ]
    }
   ],
   "source": [
    "# Performance optimization techniques\n",
    "class PerformanceOptimizer:\n",
    "    \"\"\"Utilities for optimizing model performance.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def benchmark_model(model, test_data, num_iterations=100):\n",
    "        \"\"\"Benchmark model inference speed.\"\"\"\n",
    "        print(f\"Benchmarking model with {num_iterations} iterations...\")\n",
    "        \n",
    "        latencies = []\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Simulate prediction\n",
    "            try:\n",
    "                if hasattr(model, 'predict'):\n",
    "                    _ = model.predict(test_data)\n",
    "                else:\n",
    "                    time.sleep(0.001)  # Simulate processing time\n",
    "            except Exception:\n",
    "                # If model is not fitted or has issues, simulate processing time\n",
    "                time.sleep(0.001)\n",
    "            \n",
    "            latency_ms = (time.time() - start_time) * 1000\n",
    "            latencies.append(latency_ms)\n",
    "        \n",
    "        latencies.sort()\n",
    "        \n",
    "        return {\n",
    "            'mean_latency_ms': sum(latencies) / len(latencies),\n",
    "            'median_latency_ms': latencies[len(latencies) // 2],\n",
    "            'p95_latency_ms': latencies[int(0.95 * len(latencies))],\n",
    "            'p99_latency_ms': latencies[int(0.99 * len(latencies))],\n",
    "            'min_latency_ms': min(latencies),\n",
    "            'max_latency_ms': max(latencies)\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def optimize_batch_size(model, test_data, batch_sizes=[1, 8, 16, 32, 64]):\n",
    "        \"\"\"Find optimal batch size for throughput.\"\"\"\n",
    "        print(\"Optimizing batch size...\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"Testing batch size: {batch_size}\")\n",
    "            \n",
    "            # Simulate batch processing\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Process multiple batches\n",
    "            for _ in range(10):\n",
    "                try:\n",
    "                    if hasattr(model, 'predict'):\n",
    "                        _ = model.predict(test_data[:batch_size] if len(test_data) >= batch_size else test_data)\n",
    "                    else:\n",
    "                        time.sleep(0.001 * batch_size)  # Simulate processing time\n",
    "                except Exception:\n",
    "                    # If model is not fitted, simulate processing time\n",
    "                    time.sleep(0.001 * batch_size)\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            throughput = (10 * batch_size) / total_time  # samples per second\n",
    "            \n",
    "            results[batch_size] = {\n",
    "                'throughput_samples_per_sec': throughput,\n",
    "                'avg_latency_per_sample_ms': (total_time / (10 * batch_size)) * 1000\n",
    "            }\n",
    "        \n",
    "        # Find optimal batch size\n",
    "        optimal_batch_size = max(results.keys(), key=lambda x: results[x]['throughput_samples_per_sec'])\n",
    "        \n",
    "        return {\n",
    "            'results': results,\n",
    "            'optimal_batch_size': optimal_batch_size,\n",
    "            'optimal_throughput': results[optimal_batch_size]['throughput_samples_per_sec']\n",
    "        }\n",
    "\n",
    "# Run performance benchmarks\n",
    "optimizer = PerformanceOptimizer()\n",
    "\n",
    "# Benchmark single prediction latency\n",
    "test_data = [[0.5, 0.3, 0.8, 0.1] for _ in range(10)]\n",
    "latency_results = optimizer.benchmark_model(model, test_data)\n",
    "\n",
    "print(\"=== Latency Benchmark Results ===\")\n",
    "for metric, value in latency_results.items():\n",
    "    print(f\"{metric}: {value:.2f}\")\n",
    "\n",
    "# Optimize batch size\n",
    "batch_results = optimizer.optimize_batch_size(model, test_data)\n",
    "\n",
    "print(\"\\n=== Batch Size Optimization ===\")\n",
    "print(f\"Optimal batch size: {batch_results['optimal_batch_size']}\")\n",
    "print(f\"Optimal throughput: {batch_results['optimal_throughput']:.2f} samples/sec\")\n",
    "\n",
    "print(\"\\nThroughput by batch size:\")\n",
    "for batch_size, metrics in batch_results['results'].items():\n",
    "    print(f\"  Batch {batch_size}: {metrics['throughput_samples_per_sec']:.2f} samples/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accd99a8",
   "metadata": {},
   "source": [
    "## 6. CI/CD Integration {#cicd}\n",
    "\n",
    "Integrating model deployment with CI/CD pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80dec005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting deployment pipeline for model version 1.2.3...\n",
      "Validating model performance...\n",
      "Validating model compatibility...\n",
      "Creating deployment artifacts for version 1.2.3...\n",
      "Deploying version 1.2.3 to staging...\n",
      "=== Deployment Pipeline Results ===\n",
      "Status: SUCCESS\n",
      "Message: Model version 1.2.3 deployed successfully\n",
      "Version: 1.2.3\n",
      "\n",
      "Deployment Steps:\n",
      "  performance_validation: ✅ PASSED\n",
      "    Model score 0.850 ≥ threshold 0.800\n",
      "  compatibility_validation: ✅ PASSED\n",
      "    All compatibility checks passed\n",
      "  create_artifacts: ✅ PASSED\n",
      "    Artifacts created in /home/rlfowler/Documents/myprojects/DLT/models/deployments/v1.2.3\n",
      "  deploy_staging: ✅ PASSED\n",
      "    Version 1.2.3 deployed to staging\n",
      "=== Deployment Pipeline Results ===\n",
      "Status: SUCCESS\n",
      "Message: Model version 1.2.3 deployed successfully\n",
      "Version: 1.2.3\n",
      "\n",
      "Deployment Steps:\n",
      "  performance_validation: ✅ PASSED\n",
      "    Model score 0.850 ≥ threshold 0.800\n",
      "  compatibility_validation: ✅ PASSED\n",
      "    All compatibility checks passed\n",
      "  create_artifacts: ✅ PASSED\n",
      "    Artifacts created in /home/rlfowler/Documents/myprojects/DLT/models/deployments/v1.2.3\n",
      "  deploy_staging: ✅ PASSED\n",
      "    Version 1.2.3 deployed to staging\n"
     ]
    }
   ],
   "source": [
    "# CI/CD Pipeline Components\n",
    "\n",
    "class ModelValidator:\n",
    "    \"\"\"Validate models before deployment.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "    \n",
    "    def validate_model_performance(self, model, test_data, performance_threshold):\n",
    "        \"\"\"Validate model meets performance requirements.\"\"\"\n",
    "        print(\"Validating model performance...\")\n",
    "        \n",
    "        # Simulate performance validation\n",
    "        try:\n",
    "            if hasattr(model, 'score'):\n",
    "                # For models with score method\n",
    "                score = model.score(test_data, [1] * len(test_data))\n",
    "            else:\n",
    "                # Mock score for demonstration\n",
    "                score = 0.85\n",
    "            \n",
    "            passed = score >= performance_threshold\n",
    "            \n",
    "            return {\n",
    "                'passed': passed,\n",
    "                'score': score,\n",
    "                'threshold': performance_threshold,\n",
    "                'message': f\"Model score {score:.3f} {'≥' if passed else '<'} threshold {performance_threshold:.3f}\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'passed': False,\n",
    "                'error': str(e),\n",
    "                'message': f\"Performance validation failed: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def validate_model_compatibility(self, model):\n",
    "        \"\"\"Validate model is compatible with production environment.\"\"\"\n",
    "        print(\"Validating model compatibility...\")\n",
    "        \n",
    "        checks = {\n",
    "            'has_predict_method': hasattr(model, 'predict') or hasattr(model, '__call__'),\n",
    "            'serializable': self._test_serialization(model),\n",
    "            'memory_efficient': self._test_memory_usage(model)\n",
    "        }\n",
    "        \n",
    "        all_passed = all(checks.values())\n",
    "        \n",
    "        return {\n",
    "            'passed': all_passed,\n",
    "            'checks': checks,\n",
    "            'message': 'All compatibility checks passed' if all_passed else 'Some compatibility checks failed'\n",
    "        }\n",
    "    \n",
    "    def _test_serialization(self, model):\n",
    "        \"\"\"Test if model can be serialized and deserialized.\"\"\"\n",
    "        try:\n",
    "            # Test joblib serialization\n",
    "            import tempfile\n",
    "            with tempfile.NamedTemporaryFile() as tmp:\n",
    "                joblib.dump(model, tmp.name)\n",
    "                _ = joblib.load(tmp.name)\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    def _test_memory_usage(self, model):\n",
    "        \"\"\"Test model memory usage.\"\"\"\n",
    "        try:\n",
    "            # Simple memory test - in practice, use memory profilers\n",
    "            import sys\n",
    "            size_mb = sys.getsizeof(model) / (1024 * 1024)\n",
    "            return size_mb < 1000  # Less than 1GB\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "class DeploymentManager:\n",
    "    \"\"\"Manage model deployments.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.validator = ModelValidator(config)\n",
    "    \n",
    "    def deploy_model(self, model, version, test_data):\n",
    "        \"\"\"Deploy model with validation pipeline.\"\"\"\n",
    "        print(f\"Starting deployment pipeline for model version {version}...\")\n",
    "        \n",
    "        deployment_result = {\n",
    "            'version': version,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'status': 'failed',\n",
    "            'steps': []\n",
    "        }\n",
    "        \n",
    "        # Step 1: Validate performance\n",
    "        perf_result = self.validator.validate_model_performance(model, test_data, 0.8)\n",
    "        deployment_result['steps'].append(('performance_validation', perf_result))\n",
    "        \n",
    "        if not perf_result['passed']:\n",
    "            deployment_result['message'] = 'Deployment failed: Performance validation'\n",
    "            return deployment_result\n",
    "        \n",
    "        # Step 2: Validate compatibility\n",
    "        compat_result = self.validator.validate_model_compatibility(model)\n",
    "        deployment_result['steps'].append(('compatibility_validation', compat_result))\n",
    "        \n",
    "        if not compat_result['passed']:\n",
    "            deployment_result['message'] = 'Deployment failed: Compatibility validation'\n",
    "            return deployment_result\n",
    "        \n",
    "        # Step 3: Create deployment artifacts\n",
    "        artifacts_result = self._create_deployment_artifacts(model, version)\n",
    "        deployment_result['steps'].append(('create_artifacts', artifacts_result))\n",
    "        \n",
    "        if not artifacts_result['passed']:\n",
    "            deployment_result['message'] = 'Deployment failed: Artifact creation'\n",
    "            return deployment_result\n",
    "        \n",
    "        # Step 4: Deploy to staging (simulation)\n",
    "        staging_result = self._deploy_to_staging(version)\n",
    "        deployment_result['steps'].append(('deploy_staging', staging_result))\n",
    "        \n",
    "        if not staging_result['passed']:\n",
    "            deployment_result['message'] = 'Deployment failed: Staging deployment'\n",
    "            return deployment_result\n",
    "        \n",
    "        deployment_result['status'] = 'success'\n",
    "        deployment_result['message'] = f'Model version {version} deployed successfully'\n",
    "        \n",
    "        return deployment_result\n",
    "    \n",
    "    def _create_deployment_artifacts(self, model, version):\n",
    "        \"\"\"Create deployment artifacts.\"\"\"\n",
    "        try:\n",
    "            print(f\"Creating deployment artifacts for version {version}...\")\n",
    "            \n",
    "            # Create version directory\n",
    "            version_dir = Path(f'/home/rlfowler/Documents/myprojects/DLT/models/deployments/v{version}')\n",
    "            version_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Save model\n",
    "            model_path = version_dir / 'model.joblib'\n",
    "            joblib.dump(model, model_path)\n",
    "            \n",
    "            # Save deployment metadata\n",
    "            metadata = {\n",
    "                'version': version,\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'model_file': 'model.joblib',\n",
    "                'deployment_config': self.config\n",
    "            }\n",
    "            \n",
    "            metadata_path = version_dir / 'deployment_metadata.json'\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                json.dump(metadata, f, indent=2)\n",
    "            \n",
    "            return {\n",
    "                'passed': True,\n",
    "                'artifacts_path': str(version_dir),\n",
    "                'message': f'Artifacts created in {version_dir}'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'passed': False,\n",
    "                'error': str(e),\n",
    "                'message': f'Artifact creation failed: {str(e)}'\n",
    "            }\n",
    "    \n",
    "    def _deploy_to_staging(self, version):\n",
    "        \"\"\"Deploy to staging environment.\"\"\"\n",
    "        try:\n",
    "            print(f\"Deploying version {version} to staging...\")\n",
    "            \n",
    "            # Simulate staging deployment\n",
    "            time.sleep(1)  # Simulate deployment time\n",
    "            \n",
    "            return {\n",
    "                'passed': True,\n",
    "                'staging_url': f'https://staging.api.example.com/v{version}',\n",
    "                'message': f'Version {version} deployed to staging'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'passed': False,\n",
    "                'error': str(e),\n",
    "                'message': f'Staging deployment failed: {str(e)}'\n",
    "            }\n",
    "\n",
    "# Test the deployment pipeline\n",
    "deployment_manager = DeploymentManager(production_config)\n",
    "test_data = [[0.5, 0.3, 0.8, 0.1] for _ in range(10)]\n",
    "\n",
    "deployment_result = deployment_manager.deploy_model(model, \"1.2.3\", test_data)\n",
    "\n",
    "print(\"=== Deployment Pipeline Results ===\")\n",
    "print(f\"Status: {deployment_result['status'].upper()}\")\n",
    "print(f\"Message: {deployment_result['message']}\")\n",
    "print(f\"Version: {deployment_result['version']}\")\n",
    "print(\"\\nDeployment Steps:\")\n",
    "\n",
    "for step_name, step_result in deployment_result['steps']:\n",
    "    status = \"✅ PASSED\" if step_result['passed'] else \"❌ FAILED\"\n",
    "    print(f\"  {step_name}: {status}\")\n",
    "    if 'message' in step_result:\n",
    "        print(f\"    {step_result['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81611abe",
   "metadata": {},
   "source": [
    "## 7. Best Practices {#best-practices}\n",
    "\n",
    "Summary of production deployment best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db8b095a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PRODUCTION ML DEPLOYMENT BEST PRACTICES ===\n",
      "\n",
      "Implement these practices to ensure robust, scalable, and maintainable ML systems:\n",
      "\n",
      "## Model Management\n",
      "  ✅ Version all models with semantic versioning\n",
      "  ✅ Store model metadata alongside model files\n",
      "  ✅ Use reproducible training processes\n",
      "  ✅ Implement model validation before deployment\n",
      "  ✅ Keep model registry updated\n",
      "\n",
      "## Infrastructure\n",
      "  ✅ Use containerization (Docker) for consistency\n",
      "  ✅ Implement horizontal scaling capabilities\n",
      "  ✅ Set up load balancing for high availability\n",
      "  ✅ Use environment-specific configurations\n",
      "  ✅ Implement circuit breakers for fault tolerance\n",
      "\n",
      "## Monitoring & Observability\n",
      "  ✅ Monitor prediction latency and throughput\n",
      "  ✅ Track model performance metrics\n",
      "  ✅ Implement data drift detection\n",
      "  ✅ Set up alerting for anomalies\n",
      "  ✅ Log predictions for debugging (when privacy allows)\n",
      "\n",
      "## Security\n",
      "  ✅ Implement authentication and authorization\n",
      "  ✅ Use API rate limiting\n",
      "  ✅ Encrypt data in transit and at rest\n",
      "  ✅ Validate and sanitize all inputs\n",
      "  ✅ Follow data privacy regulations\n",
      "\n",
      "## Performance\n",
      "  ✅ Optimize batch sizes for throughput\n",
      "  ✅ Use model quantization when appropriate\n",
      "  ✅ Implement caching for frequent predictions\n",
      "  ✅ Profile and optimize memory usage\n",
      "  ✅ Use async processing for non-real-time workloads\n",
      "\n",
      "## CI/CD\n",
      "  ✅ Automate testing and validation pipelines\n",
      "  ✅ Implement canary deployments\n",
      "  ✅ Use blue-green deployment strategies\n",
      "  ✅ Automate rollback procedures\n",
      "  ✅ Test in staging before production\n",
      "\n",
      "\n",
      "=== DEPLOYMENT READINESS ASSESSMENT ===\n",
      "\n",
      "Use this checklist to assess your deployment readiness:\n",
      "\n",
      "1. □ Model performance meets business requirements\n",
      "2. □ Model is tested on production-like data\n",
      "3. □ Infrastructure can handle expected load\n",
      "4. □ Monitoring and alerting are configured\n",
      "5. □ Security measures are implemented\n",
      "6. □ Rollback procedure is tested\n",
      "7. □ Documentation is complete\n",
      "8. □ Team is trained on operations\n",
      "\n",
      "✅ All items checked = Ready for production deployment!\n"
     ]
    }
   ],
   "source": [
    "# Production Best Practices Checklist\n",
    "best_practices = {\n",
    "    \"Model Management\": [\n",
    "        \"✅ Version all models with semantic versioning\",\n",
    "        \"✅ Store model metadata alongside model files\",\n",
    "        \"✅ Use reproducible training processes\",\n",
    "        \"✅ Implement model validation before deployment\",\n",
    "        \"✅ Keep model registry updated\"\n",
    "    ],\n",
    "    \"Infrastructure\": [\n",
    "        \"✅ Use containerization (Docker) for consistency\",\n",
    "        \"✅ Implement horizontal scaling capabilities\",\n",
    "        \"✅ Set up load balancing for high availability\",\n",
    "        \"✅ Use environment-specific configurations\",\n",
    "        \"✅ Implement circuit breakers for fault tolerance\"\n",
    "    ],\n",
    "    \"Monitoring & Observability\": [\n",
    "        \"✅ Monitor prediction latency and throughput\",\n",
    "        \"✅ Track model performance metrics\",\n",
    "        \"✅ Implement data drift detection\",\n",
    "        \"✅ Set up alerting for anomalies\",\n",
    "        \"✅ Log predictions for debugging (when privacy allows)\"\n",
    "    ],\n",
    "    \"Security\": [\n",
    "        \"✅ Implement authentication and authorization\",\n",
    "        \"✅ Use API rate limiting\",\n",
    "        \"✅ Encrypt data in transit and at rest\",\n",
    "        \"✅ Validate and sanitize all inputs\",\n",
    "        \"✅ Follow data privacy regulations\"\n",
    "    ],\n",
    "    \"Performance\": [\n",
    "        \"✅ Optimize batch sizes for throughput\",\n",
    "        \"✅ Use model quantization when appropriate\",\n",
    "        \"✅ Implement caching for frequent predictions\",\n",
    "        \"✅ Profile and optimize memory usage\",\n",
    "        \"✅ Use async processing for non-real-time workloads\"\n",
    "    ],\n",
    "    \"CI/CD\": [\n",
    "        \"✅ Automate testing and validation pipelines\",\n",
    "        \"✅ Implement canary deployments\",\n",
    "        \"✅ Use blue-green deployment strategies\",\n",
    "        \"✅ Automate rollback procedures\",\n",
    "        \"✅ Test in staging before production\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"=== PRODUCTION ML DEPLOYMENT BEST PRACTICES ===\")\n",
    "print(\"\\nImplement these practices to ensure robust, scalable, and maintainable ML systems:\")\n",
    "print()\n",
    "\n",
    "for category, practices in best_practices.items():\n",
    "    print(f\"## {category}\")\n",
    "    for practice in practices:\n",
    "        print(f\"  {practice}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n=== DEPLOYMENT READINESS ASSESSMENT ===\")\n",
    "print(\"\\nUse this checklist to assess your deployment readiness:\")\n",
    "print()\n",
    "\n",
    "assessment_items = [\n",
    "    \"Model performance meets business requirements\",\n",
    "    \"Model is tested on production-like data\",\n",
    "    \"Infrastructure can handle expected load\",\n",
    "    \"Monitoring and alerting are configured\",\n",
    "    \"Security measures are implemented\",\n",
    "    \"Rollback procedure is tested\",\n",
    "    \"Documentation is complete\",\n",
    "    \"Team is trained on operations\"\n",
    "]\n",
    "\n",
    "for i, item in enumerate(assessment_items, 1):\n",
    "    print(f\"{i}. □ {item}\")\n",
    "\n",
    "print(\"\\n✅ All items checked = Ready for production deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08090b27",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered the essential aspects of deploying DLT models to production:\n",
    "\n",
    "### Key Topics Covered:\n",
    "1. **Model Serialization** - Different methods for saving and loading models\n",
    "2. **Production Configuration** - Environment-specific settings and security\n",
    "3. **Serving Strategies** - REST API and batch prediction services\n",
    "4. **Monitoring** - Comprehensive metrics tracking and alerting\n",
    "5. **Performance Optimization** - Benchmarking and optimization techniques\n",
    "6. **CI/CD Integration** - Automated validation and deployment pipelines\n",
    "7. **Best Practices** - Production deployment checklist\n",
    "\n",
    "### Next Steps:\n",
    "- Adapt the code examples to your specific model types and frameworks\n",
    "- Set up proper infrastructure (containers, orchestration, monitoring)\n",
    "- Implement security measures appropriate for your use case\n",
    "- Test thoroughly in staging environments before production\n",
    "- Establish monitoring and incident response procedures\n",
    "\n",
    "### Additional Resources:\n",
    "- Review the other notebooks for model development guidance\n",
    "- Consult framework-specific documentation for advanced features\n",
    "- Consider using MLOps platforms for enterprise deployments\n",
    "\n",
    "Remember: Production ML is not just about the model - it's about building reliable, scalable, and maintainable systems that deliver value to users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c96cadce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cell - Notebook kernel is running!\n",
      "Working directory: /home/rlfowler/Documents/myprojects/DLT/notebooks\n",
      "Python version: 3.10.18 (main, Jun  4 2025, 08:56:00) [GCC 9.4.0]\n",
      "Basic imports working!\n"
     ]
    }
   ],
   "source": [
    "print(\"Test cell - Notebook kernel is running!\")\n",
    "import os\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Python version: {os.sys.version}\")\n",
    "print(\"Basic imports working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b36ce248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Basic production deployment imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Basic imports for production deployment\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import joblib\n",
    "import logging\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"✅ Basic production deployment imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f201e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mock model and config created for demonstration!\n",
      "Model type: RandomForestClassifier\n",
      "Model score: 0.990\n"
     ]
    }
   ],
   "source": [
    "# Create a mock model and config for demonstration\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create sample data and train a mock model\n",
    "X, y = make_classification(n_samples=100, n_features=10, n_informative=8, n_classes=3, random_state=42)\n",
    "model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Create mock config\n",
    "class MockConfig:\n",
    "    def get(self, key, default=None):\n",
    "        configs = {\n",
    "            'model': {'type': 'sklearn'},\n",
    "            'experiment': {'name': 'mock_experiment'}\n",
    "        }\n",
    "        return configs.get(key, default)\n",
    "\n",
    "config = MockConfig()\n",
    "\n",
    "print(\"✅ Mock model and config created for demonstration!\")\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"Model score: {model.score(X, y):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc9baca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Production config and model path set up!\n"
     ]
    }
   ],
   "source": [
    "# Create production config for the demo\n",
    "production_config = {\n",
    "    'logging': {\n",
    "        'level': 'INFO',\n",
    "        'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    },\n",
    "    'model_serving': {\n",
    "        'batch_size': 64,\n",
    "        'max_latency_ms': 100\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create model path for reference\n",
    "model_path = Path('/home/rlfowler/Documents/myprojects/DLT/models/production/model.joblib')\n",
    "\n",
    "print(\"✅ Production config and model path set up!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
